{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF_IDF_NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpY-ArzB1rH-",
        "colab_type": "text"
      },
      "source": [
        "# **Disease Detection using Symptoms and Treatment recommendation**\n",
        "\n",
        "This notebook contains the application of Neural Net and GAN on the disease dataset generated through scrapping.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etxuEgYCG7bC",
        "colab_type": "code",
        "outputId": "b1aa83f5-0ac1-4c9b-b532-575fc01660c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# importing nltk to download resources for stopwords and wordnet\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrLG2ksh5w4Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing all libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "import math\n",
        "import operator\n",
        "import pickle\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from statistics import mean\n",
        "from nltk.corpus import wordnet \n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from itertools import combinations\n",
        "from time import time\n",
        "from collections import Counter\n",
        "import operator\n",
        "import warnings\n",
        "from Treatment import diseaseDetail\n",
        "# ignore warnings generated due to usage of old version of tensorflow\n",
        "warnings.simplefilter(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1sbUx8C22zG",
        "colab_type": "text"
      },
      "source": [
        "**Disease Symptom dataset** was created in a separate python program.\n",
        "\n",
        "**Dataset scrapping** was done using **NHP website** and **wikipedia data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txTu6XeVgGAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Dataset scraped from NHP (https://www.nhp.gov.in/disease-a-z) & Wikipedia\n",
        "# Scrapping and creation of dataset csv is done in a separate program\n",
        "df=pd.read_csv(\"/content/drive/My Drive/Python Project data/IR_Project/dis_sym_dataset_norm.csv\")\n",
        "documentname_list=list(df['label_dis'])\n",
        "df=df.iloc[:,1:]\n",
        "columns_name=list(df.columns)\n",
        "documentname_list=list(documentname_list)\n",
        "\n",
        "N=len(df)\n",
        "M=len(columns_name)\n",
        "\n",
        "# All symptoms IDF\n",
        "idf={}\n",
        "for col in columns_name:\n",
        "  temp=np.count_nonzero(df[col])\n",
        "  idf[col]=np.log(N/temp)\n",
        "\n",
        "# All disease,symptom TF\n",
        "tf={}\n",
        "for i in range(N):\n",
        "  for col in columns_name:\n",
        "    key=(documentname_list[i],col)\n",
        "    tf[key]=df.loc[i,col]\n",
        "\n",
        "# All disease,symptom TF.IDF\n",
        "tf_idf={}\n",
        "for i in range(N):\n",
        "  for col in columns_name:\n",
        "    key=(documentname_list[i],col)\n",
        "    tf_idf[key]=float(idf[col])*float(tf[key])\n",
        "\n",
        "# vector of TF.IDF\n",
        "D = np.zeros((N, M),dtype='float32')\n",
        "for i in tf_idf:\n",
        "    sym = columns_name.index(i[1])\n",
        "    dis=documentname_list.index(i[0])\n",
        "    D[dis][sym] = tf_idf[i]\n",
        "\n",
        "# function for cosine dot product\n",
        "def cosine_dot(a, b):\n",
        "    if np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        temp = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "        return temp\n",
        "\n",
        "# convert data to lower case\n",
        "def convert_tolowercase(data):\n",
        "    return data.lower()\n",
        "\n",
        "# tokenizing using regextokenizer\n",
        "def regextokenizer_func(data):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    data = tokenizer.tokenize(data)\n",
        "    return data\n",
        "\n",
        "# function to generate query vector for tf_idf\n",
        "def gen_vector(tokens):\n",
        "    Q = np.zeros(M)\n",
        "    counter = Counter(tokens)\n",
        "    query_weights = {}\n",
        "    for token in np.unique(tokens):\n",
        "        tf = counter[token]\n",
        "        try:\n",
        "          idf_temp=idf[token]\n",
        "        except:\n",
        "          pass\n",
        "        try:\n",
        "            ind = columns_name.index(token)\n",
        "            Q[ind] = tf*idf_temp\n",
        "        except:\n",
        "            pass\n",
        "    return Q\n",
        "\n",
        "# function to calculate tf_idf_score\n",
        "def tf_idf_score(k, query):\n",
        "    query_weights = {}\n",
        "    for key in tf_idf:\n",
        "        if key[1] in query:\n",
        "            try:\n",
        "                query_weights[key[0]] += tf_idf[key]\n",
        "            except:\n",
        "                query_weights[key[0]] = tf_idf[key]\n",
        "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
        "  \n",
        "    l = []\n",
        "    for i in query_weights[:k]:\n",
        "        l.append(i)\n",
        "    return l\n",
        "\n",
        "# function to calculte Cosine Similarity \n",
        "def cosine_similarity(k, query):\n",
        "    d_cosines = []\n",
        "    query_vector = gen_vector(query)\n",
        "    for d in D:\n",
        "        d_cosines.append(cosine_dot(query_vector, d))\n",
        "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
        "  \n",
        "    final_display_disease={}\n",
        "    for lt in set(out):\n",
        "      final_display_disease[lt] = float(d_cosines[lt])\n",
        "    return final_display_disease"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyd1WbBq5Ngh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# returns the list of synonyms of the input word from thesaurus.com (https://www.thesaurus.com/) and wordnet (https://www.nltk.org/howto/wordnet.html)\n",
        "def synonyms(term):\n",
        "    synonyms = []\n",
        "    response = requests.get('https://www.thesaurus.com/browse/{}'.format(term))\n",
        "    soup = BeautifulSoup(response.content,  \"html.parser\")\n",
        "    try:\n",
        "        container=soup.find('section', {'class': 'MainContentContainer'}) \n",
        "        row=container.find('div',{'class':'css-191l5o0-ClassicContentCard'})\n",
        "        row = row.find_all('li')\n",
        "        for x in row:\n",
        "            synonyms.append(x.get_text())\n",
        "    except:\n",
        "        None\n",
        "    for syn in wordnet.synsets(term):\n",
        "        synonyms+=syn.lemma_names()\n",
        "    return set(synonyms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXPUlgHi63Zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiate objects of libraries\n",
        "splitter = RegexpTokenizer(r'\\w+')\n",
        "stop_words = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DGtPYRLhey0y"
      },
      "source": [
        "**Disease Symptom dataset** was created in a separate python program.\n",
        "\n",
        "**Dataset scrapping** was done using **NHP website** and **wikipedia data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZTXyRhNgN_O",
        "colab_type": "text"
      },
      "source": [
        "Disease Combination dataset contains the combinations for each of the disease present in dataset as practically it is often observed that it is not necessary for a person to have a disease when all the symptoms are faced by the patient or the user.\n",
        "\n",
        "*To tackle this problem, combinations are made with the symptoms for each disease.*\n",
        "\n",
        " **This increases the size of the data exponentially and helps the model to predict the disease with much better accuracy.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1LSI08aiDTn",
        "colab_type": "text"
      },
      "source": [
        "*df_comb -> Dataframe consisting of dataset generated by combining symptoms for each disease.*\n",
        "\n",
        "*df_norm -> Dataframe consisting of dataset which contains a single row for each diseases with all the symptoms for that corresponding disease.*\n",
        "\n",
        "**Dataset contains 261 diseases and their symptoms**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpK73qQx5NmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Dataset scraped from NHP (https://www.nhp.gov.in/disease-a-z) & Wikipedia\n",
        "# Scrapping and creation of dataset csv is done in a separate program\n",
        "df_comb = pd.read_csv(\"/content/drive/My Drive/Python Project data/IR_Project/dis_sym_dataset_comb.csv\") # Disease combination\n",
        "df_norm = pd.read_csv(\"/content/drive/My Drive/Python Project data/IR_Project/dis_sym_dataset_norm.csv\") # Individual Disease\n",
        "Y = df_norm.iloc[:, 0:1]\n",
        "X = df_norm.iloc[:, 1:]\n",
        "# List of symptoms\n",
        "dataset_symptoms = list(X.columns)\n",
        "diseases = list(set(Y['label_dis']))\n",
        "diseases.sort()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd8K2QeH5NsL",
        "colab_type": "code",
        "outputId": "62571279-7471-4698-f233-6e0a97c34979",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# Taking symptoms from user as input\n",
        "# Preprocessing the input symtoms \n",
        "user_symptoms = str(input(\"\\nPlease enter symptoms separated by comma(,):\\n\")).lower().split(',')\n",
        "processed_user_symptoms=[]\n",
        "for sym in user_symptoms:\n",
        "    sym=sym.strip()\n",
        "    sym=sym.replace('-',' ')\n",
        "    sym=sym.replace(\"'\",'')\n",
        "    sym = ' '.join([lemmatizer.lemmatize(word) for word in splitter.tokenize(sym)])\n",
        "    processed_user_symptoms.append(sym)  "
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Please enter symptoms separated by comma(,):\n",
            "coughing,pyrexia,tire,loss of smell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THRf6-Wa5Nxu",
        "colab_type": "code",
        "outputId": "122d0bcb-569f-4610-f713-76382d03fb3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Taking each user symptom and finding all its synonyms and appending it to the pre-processed symptom string\n",
        "user_symptoms = []\n",
        "for user_sym in processed_user_symptoms:\n",
        "    user_sym = user_sym.split()\n",
        "    str_sym = set()\n",
        "    for comb in range(1, len(user_sym)+1):\n",
        "        for subset in combinations(user_sym, comb):\n",
        "            subset=' '.join(subset)\n",
        "            subset = synonyms(subset) \n",
        "            str_sym.update(subset)\n",
        "    str_sym.add(' '.join(user_sym))\n",
        "    user_symptoms.append(' '.join(str_sym).replace('_',' '))\n",
        "# query expansion performed by joining synonyms found for each symptoms initially entered\n",
        "print(\"After query expansion done by using the symptoms entered\")\n",
        "print(user_symptoms)"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After query expansion done by using the symptoms entered\n",
            "['expectorate spit up hawk clear throat coughing convulse hack cough choke bark hem vomit whoop', 'climate feverishness febrility body heat fever incalescence degrees febricity cold heat calefaction warmth pyrexia condition thermal reading', 'irk collapse irritate distress fag burn out exhaust fail wear down exasperate sicken wilt enervate poop out debilitate tire wear weaken overburden prostrate fag out deject droop run down ennui tax go stale weary grow weary bore peter out disgust sink flag wear out wear upon tyre yawn faint overtax bush drain fatigue drop dispirit tire out play out pain pall fold annoy wear down sap wear out crawl overwork strain overstrain depress displease give out worry vex harass dishearten jade put to sleep outwear nauseate', 'deprivation incense defeat privation tone destruction trail stink want in reference to disappearance olfactory sensation destitution personnel casualty emanation concerning death harm debit disaster casualty accident bad luck connected with olfactory modality red ink spirit smelling red exit damage appertaining to disadvantage going from detriment scent cataclysm sense smack misplacing calamity out of epithetical impairment mishap olfactory property odour retardation showing loss of smell injury like passing depletion aroma waste forfeiture bouquet reek flavour about savor departure fall made from proceeding from tang related to feel expiration whiff need sense of smell as regards pertaining to spice smell out fragrance going loss wreckage sacrifice mislaying flavor release olfactory perception referring to out from characterized by perfume smell trouble odor catastrophe based on stench trial regarding feeling cost shrinkage as concerns olfaction in regard to essence misadventure perdition peculiar to failure dispossession bereavement coming from away from losing redolence debt ruin squandering consisting of appropriate to undoing attributed to look deficiency belonging to fatality containing trace hurt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sPyVlJIjdv2",
        "colab_type": "text"
      },
      "source": [
        "The below procedure is performed in order to show the symptom synonmys found for the symptoms entered by the user.\n",
        "\n",
        "The symptom synonyms and user symptoms are matched with the symptoms present in dataset. Only the symptoms which matches the symptoms present in dataset are shown back to the user. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFUYSnLU5Nu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loop over all the symptoms in dataset and check its similarity score to the synonym string of the user-input \n",
        "# symptoms. If similarity>0.5, add the symptom to the final list\n",
        "found_symptoms = set()\n",
        "for idx, data_sym in enumerate(dataset_symptoms):\n",
        "    data_sym_split=data_sym.split()\n",
        "    for user_sym in user_symptoms:\n",
        "        count=0\n",
        "        for symp in data_sym_split:\n",
        "            if symp in user_sym.split():\n",
        "                count+=1\n",
        "        if count/len(data_sym_split)>0.5:\n",
        "            found_symptoms.add(data_sym)\n",
        "found_symptoms = list(found_symptoms)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBV1qPFv5NpY",
        "colab_type": "code",
        "outputId": "bc96a52e-15fc-496a-c333-5d195a4fe430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "# Print all found symptoms\n",
        "print(\"Top matching symptoms from your search!\")\n",
        "for idx, symp in enumerate(found_symptoms):\n",
        "    print(idx,\":\",symp)\n",
        "\n",
        "# Show the related symptoms found in the dataset and ask user to select among them\n",
        "select_list = input(\"\\nPlease select the relevant symptoms. Enter indices (separated-space):\\n\").split()\n",
        "\n",
        "# Find other relevant symptoms from the dataset based on user symptoms based on the highest co-occurance with the\n",
        "# ones that is input by the user\n",
        "dis_list = set()\n",
        "final_symp = [] \n",
        "counter_list = []\n",
        "for idx in select_list:\n",
        "    symp=found_symptoms[int(idx)]\n",
        "    final_symp.append(symp)\n",
        "    dis_list.update(set(df_norm[df_norm[symp]==1]['label_dis']))\n",
        "   \n",
        "for dis in dis_list:\n",
        "    row = df_norm.loc[df_norm['label_dis'] == dis].values.tolist()\n",
        "    row[0].pop(0)\n",
        "    for idx,val in enumerate(row[0]):\n",
        "        if val!=0 and dataset_symptoms[idx] not in final_symp:\n",
        "            counter_list.append(dataset_symptoms[idx])\n",
        "\n",
        "# Symptoms that co-occur with the ones selected by user              \n",
        "dict_symp = dict(Counter(counter_list))\n",
        "dict_symp_tup = sorted(dict_symp.items(), key=operator.itemgetter(1),reverse=True)   "
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top matching symptoms from your search!\n",
            "0 : coughing\n",
            "1 : fatigue\n",
            "2 : feeling like passing\n",
            "3 : fever\n",
            "4 : crawl\n",
            "5 : feeling need urinate right away\n",
            "6 : red\n",
            "7 : trouble sensation\n",
            "8 : loss smell\n",
            "\n",
            "Please select the relevant symptoms. Enter indices (separated-space):\n",
            "0 1 3 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgvzIn7Q5NjV",
        "colab_type": "code",
        "outputId": "d0da9e18-ff49-4cd1-f486-983f06140917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "# Iteratively, suggest top co-occuring symptoms to the user and ask to select the ones applicable \n",
        "found_symptoms=[]\n",
        "count=0\n",
        "for tup in dict_symp_tup:\n",
        "    count+=1\n",
        "    found_symptoms.append(tup[0])\n",
        "    if count%5==0 or count==len(dict_symp_tup):\n",
        "        print(\"\\nCommon co-occuring symptoms:\")\n",
        "        for idx,ele in enumerate(found_symptoms):\n",
        "            print(idx,\":\",ele)\n",
        "        select_list = input(\"Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\\n\").lower().split();\n",
        "        if select_list[0]=='no':\n",
        "            break\n",
        "        if select_list[0]=='-1':\n",
        "            found_symptoms = [] \n",
        "            continue\n",
        "        for idx in select_list:\n",
        "            final_symp.append(found_symptoms[int(idx)])\n",
        "        found_symptoms = []    "
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Common co-occuring symptoms:\n",
            "0 : headache\n",
            "1 : testicular pain\n",
            "2 : vomiting\n",
            "3 : sore throat\n",
            "4 : barky cough\n",
            "Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\n",
            "-1\n",
            "\n",
            "Common co-occuring symptoms:\n",
            "0 : maculopapular rash\n",
            "1 : diarrhea\n",
            "2 : confusion\n",
            "3 : feeling tired\n",
            "4 : swollen lymph node\n",
            "Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\n",
            "-1\n",
            "\n",
            "Common co-occuring symptoms:\n",
            "0 : runny nose\n",
            "1 : shortness breath\n",
            "2 : unintended weight loss\n",
            "3 : chest pain\n",
            "4 : large lymph node\n",
            "Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\n",
            "1\n",
            "\n",
            "Common co-occuring symptoms:\n",
            "0 : tiredness\n",
            "1 : nausea\n",
            "2 : muscle weakness\n",
            "3 : seizure\n",
            "4 : hoarse voice\n",
            "Do you have have of these symptoms? If Yes, enter the indices (space-separated), 'no' to stop, '-1' to skip:\n",
            "no\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI5taHc8pfY3",
        "colab_type": "text"
      },
      "source": [
        "Final Symptom list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYPReN9D5Nd_",
        "colab_type": "code",
        "outputId": "af695963-a0e3-4e76-c2a9-4a7c2ca75442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "#Calculating TF-IDF and Cosine Similarity using matched symptoms\n",
        "k = 10\n",
        "\n",
        "print(\"Final list of Symptoms used for prediction are : \")\n",
        "for val in final_symp:\n",
        "    print(val)"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final list of Symptoms used for prediction are : \n",
            "coughing\n",
            "fatigue\n",
            "fever\n",
            "loss smell\n",
            "shortness breath\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_A-6Dl5qHlv",
        "colab_type": "text"
      },
      "source": [
        "# **Showing the list of top k diseases to the user with their prediction probabilities.**\n",
        "\n",
        "# **For getting information about the suggested treatments, user can enter the corresponding index to know more details.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWUsVkF3t6jk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "7baabb86-a3b9-4e7b-e7ce-fc4818ea9de3"
      },
      "source": [
        "topk1=tf_idf_score(k,final_symp)\n",
        "topk2=cosine_similarity(k,final_symp)\n",
        "# Show top 10 highly probable disease to the user.\n",
        "print(f\"\\nTop {k} diseases predicted based on TF_IDF Matching :\\n\")\n",
        "i = 0\n",
        "topk1_index_mapping = {}\n",
        "for key, score in topk1:\n",
        "  print(f\"{i}. Disease : {key} \\t Score : {round(score, 2)}\")\n",
        "  topk1_index_mapping[i] = key\n",
        "  i += 1\n",
        "\n",
        "select = input(\"\\nMore details about the disease? Enter index of disease or '-1' to discontinue:\\n\")\n",
        "if select!='-1':\n",
        "    dis=topk1_index_mapping[int(select)]\n",
        "    print()\n",
        "    print(diseaseDetail(dis))"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Top 10 diseases predicted based on TF_IDF Matching :\n",
            "\n",
            "0. Disease : Coronavirus disease 2019 (COVID-19) \t Score : 13.36\n",
            "1. Disease : Asthma \t Score : 7.2\n",
            "2. Disease : Influenza \t Score : 5.75\n",
            "3. Disease : Nasal Polyps \t Score : 4.87\n",
            "4. Disease : Brucellosis \t Score : 4.47\n",
            "5. Disease : Dehydration \t Score : 4.47\n",
            "6. Disease : Mouth Breathing \t Score : 4.47\n",
            "7. Disease : Anthrax \t Score : 4.02\n",
            "8. Disease : Legionellosis \t Score : 4.02\n",
            "9. Disease : Middle East respiratory syndrome coronavirus (MERS‐CoV) \t Score : 4.02\n",
            "\n",
            "More details about the disease? Enter index of disease or '-1' to discontinue:\n",
            "-1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHiGmHhqdnMs",
        "colab_type": "code",
        "outputId": "70f583d0-83e5-4424-8849-c97fbb189c41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "source": [
        "# display top k diseases predicted with cosine probablity\n",
        "print(f\"Top {k} disease based on Cosine Similarity Matching :\\n \")\n",
        "topk2_sorted = dict(sorted(topk2.items(), key=lambda kv: kv[1], reverse=True))\n",
        "j = 0\n",
        "topk2_index_mapping = {}\n",
        "for key in topk2_sorted:\n",
        "  print(f\"{j}. Disease : {diseases[key]} \\t Score : {round(topk2_sorted[key], 2)}\")\n",
        "  topk2_index_mapping[j] = diseases[key]\n",
        "  j += 1\n",
        "\n",
        "    \n",
        "select = input(\"\\nMore details about the disease? Enter index of disease or '-1' to discontinue and close the system:\\n\")\n",
        "if select!='-1':\n",
        "    dis=topk2_index_mapping[int(select)]\n",
        "    print()\n",
        "    print(diseaseDetail(dis))"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 disease based on Cosine Similarity Matching :\n",
            " \n",
            "0. Disease : Coronavirus disease 2019 (COVID-19) \t Score : 0.64\n",
            "1. Disease : Brucellosis \t Score : 0.52\n",
            "2. Disease : Asthma \t Score : 0.34\n",
            "3. Disease : Influenza \t Score : 0.28\n",
            "4. Disease : Dehydration \t Score : 0.26\n",
            "5. Disease : Nasal Polyps \t Score : 0.24\n",
            "6. Disease : Middle East respiratory syndrome coronavirus (MERS‐CoV) \t Score : 0.24\n",
            "7. Disease : Mouth Breathing \t Score : 0.21\n",
            "8. Disease : Coronary Heart Disease \t Score : 0.21\n",
            "9. Disease : Legionellosis \t Score : 0.2\n",
            "\n",
            "More details about the disease? Enter index of disease or '-1' to discontinue and close the system:\n",
            "2\n",
            "\n",
            "Asthma\n",
            "Specialty -  Pulmonology \n",
            "Symptoms -  Recurring episodes of wheezing, coughing, chest tightness, shortness of breath   \n",
            "Duration -  Long term   \n",
            "Causes -  Genetic and environmental factors   \n",
            "Risk factors -  Air pollution, allergens   \n",
            "Diagnostic method -  Based on symptoms, response to therapy, spirometry   \n",
            "Treatment -  Avoiding triggers, inhaled corticosteroids, salbutamol   \n",
            "Frequency -  358 million (2015)   \n",
            "Deaths -  397,100 (2015)   \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohuPaboMyKXa",
        "colab_type": "text"
      },
      "source": [
        "# New Section\n",
        "**NEURAL_NETWORK AND GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9pStYfV_FGM",
        "colab_type": "code",
        "outputId": "b509778c-015e-4f20-8e65-be400cc81d20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "\n",
        "!pip install neural_structured_learning\n",
        "#importing all libraries\n",
        "import neural_structured_learning as nsl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras import initializers\n",
        "from keras.optimizers import SGD\n",
        "#import neural_structured_learning as nsl\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: neural_structured_learning in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from neural_structured_learning) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from neural_structured_learning) (1.12.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from neural_structured_learning) (19.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from neural_structured_learning) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->neural_structured_learning) (1.18.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TQqjkbmymrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reading Dataset and making dataframe\n",
        "datat=pd.read_csv('/content/drive/My Drive/Python Project data/IR_Project/dis_sym_dataset_comb.csv')\n",
        "df_new=pd.DataFrame(datat)\n",
        "df_new=df_new.sample(frac=1)\n",
        "#print(df_new)\n",
        "Y=df_new['label_dis']\n",
        "X=df_new.drop(columns='label_dis',axis=1)\n",
        "total_symptoms_len=len(X.columns)\n",
        "total_disease_len=len(set(Y))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJGNzECTynHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Label Encoding Class to numeric type \n",
        "#Converting class to categorical type for categorical cross entropy\n",
        "lb=LabelEncoder()\n",
        "Y=lb.fit_transform(Y)\n",
        "Ycat=to_categorical(Y)\n",
        "X=np.array(X)\n",
        "Y=np.array(Y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB0KNQoRynNL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing tensorflow and keras frameworks\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "#base Model Neurel Net\n",
        "def base_model():\n",
        "  inputs=keras.Input(shape=(total_symptoms_len,),dtype=tf.float32,name=IMAGE_INPUT_NAME)#defining input shape and dtype \n",
        "  x=inputs\n",
        "  x=keras.layers.Dense(1000,activation='relu',use_bias=True,kernel_initializer=initializers.he_normal(seed=None))(x)#Dense layer relu\n",
        "\n",
        "  x=keras.layers.Dense(1000,activation='relu',use_bias=True,kernel_initializer=initializers.he_normal(seed=None))(x)#Dense layer relu\n",
        "\n",
        "  outputs=keras.layers.Dense(total_disease_len,activation='softmax')(x)#output Dense layer with class size\n",
        "\n",
        "  model=keras.Model(inputs=inputs,outputs=outputs,name='NN_sequential_model')#creating model\n",
        "\n",
        "  #model.add(Dense(1500,activation='relu',kernel_initializer='he_uniform'))\n",
        "  # model.add(Dense(500,activation='relu',use_bias=True,kernel_initializer=initializers.he_normal(seed=None)))\n",
        "  # model.add(Dense(183,activation='softmax'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xYNar3nynKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def convert_to_dictionaries(image, label):\n",
        "  return {IMAGE_INPUT_NAME: image, LABEL_INPUT_NAME: label}\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHSec4ggynSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMAGE_INPUT_NAME = 'image'\n",
        "LABEL_INPUT_NAME = 'label'\n",
        "#making adversarial Configurations for training\n",
        "adv_config = nsl.configs.make_adv_reg_config(\n",
        "    multiplier=0.2,\n",
        "    adv_step_size=0.0001\n",
        ")\n",
        "base_adv_model =base_model()#calling base model\n",
        "#building adversiaral graphs for embedding and combining with base modrl\n",
        "adv_model = nsl.keras.AdversarialRegularization(\n",
        "    base_adv_model,\n",
        "    label_keys=[LABEL_INPUT_NAME],\n",
        "    adv_config=adv_config\n",
        ")\n",
        "train_set_for_adv_model = convert_to_dictionaries(X,Ycat)#converting it to dictionary for training adversiarial model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsVGBV7YynVG",
        "colab_type": "code",
        "outputId": "3094a75e-a221-47bd-f933-5f9d82052617",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "base_mod=base_model()\n",
        "base_mod.summary()\n",
        "es=tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=10)#early stopping\n",
        "mc = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)#saving best model\n",
        "print(\"Normal Feed Forward Neural Network\")\n",
        "base_mod.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "history=base_mod.fit(X,Ycat,validation_split=0.2,epochs=20,verbose=1,callbacks=[es,mc])#training Neural Network\n",
        "base_mod.summary()"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"NN_sequential_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "image (InputLayer)           [(None, 489)]             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1000)              490000    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1000)              1001000   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 261)               261261    \n",
            "=================================================================\n",
            "Total params: 1,752,261\n",
            "Trainable params: 1,752,261\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Normal Feed Forward Neural Network\n",
            "Epoch 1/20\n",
            "219/221 [============================>.] - ETA: 0s - loss: 1.5332 - accuracy: 0.7349\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.86022, saving model to best_model.h5\n",
            "221/221 [==============================] - 5s 20ms/step - loss: 1.5261 - accuracy: 0.7357 - val_loss: 0.7140 - val_accuracy: 0.8602\n",
            "Epoch 2/20\n",
            "220/221 [============================>.] - ETA: 0s - loss: 0.4895 - accuracy: 0.8902\n",
            "Epoch 00002: val_accuracy improved from 0.86022 to 0.87889, saving model to best_model.h5\n",
            "221/221 [==============================] - 4s 20ms/step - loss: 0.4886 - accuracy: 0.8904 - val_loss: 0.5300 - val_accuracy: 0.8789\n",
            "Epoch 3/20\n",
            "220/221 [============================>.] - ETA: 0s - loss: 0.3728 - accuracy: 0.8983\n",
            "Epoch 00003: val_accuracy improved from 0.87889 to 0.88568, saving model to best_model.h5\n",
            "221/221 [==============================] - 4s 20ms/step - loss: 0.3742 - accuracy: 0.8980 - val_loss: 0.4860 - val_accuracy: 0.8857\n",
            "Epoch 4/20\n",
            "220/221 [============================>.] - ETA: 0s - loss: 0.3230 - accuracy: 0.9068\n",
            "Epoch 00004: val_accuracy did not improve from 0.88568\n",
            "221/221 [==============================] - 4s 19ms/step - loss: 0.3230 - accuracy: 0.9068 - val_loss: 0.5182 - val_accuracy: 0.8800\n",
            "Epoch 5/20\n",
            "219/221 [============================>.] - ETA: 0s - loss: 0.3095 - accuracy: 0.9053\n",
            "Epoch 00005: val_accuracy did not improve from 0.88568\n",
            "221/221 [==============================] - 4s 20ms/step - loss: 0.3078 - accuracy: 0.9058 - val_loss: 0.5043 - val_accuracy: 0.8795\n",
            "Epoch 6/20\n",
            "220/221 [============================>.] - ETA: 0s - loss: 0.2887 - accuracy: 0.9065\n",
            "Epoch 00006: val_accuracy did not improve from 0.88568\n",
            "221/221 [==============================] - 4s 20ms/step - loss: 0.2891 - accuracy: 0.9065 - val_loss: 0.5035 - val_accuracy: 0.8806\n",
            "Epoch 7/20\n",
            "220/221 [============================>.] - ETA: 0s - loss: 0.2827 - accuracy: 0.9061\n",
            "Epoch 00007: val_accuracy did not improve from 0.88568\n",
            "221/221 [==============================] - 4s 20ms/step - loss: 0.2827 - accuracy: 0.9062 - val_loss: 0.5065 - val_accuracy: 0.8783\n",
            "Epoch 8/20\n",
            "220/221 [============================>.] - ETA: 0s - loss: 0.2779 - accuracy: 0.9062\n",
            "Epoch 00008: val_accuracy did not improve from 0.88568\n",
            "221/221 [==============================] - 4s 20ms/step - loss: 0.2779 - accuracy: 0.9063 - val_loss: 0.5127 - val_accuracy: 0.8800\n",
            "Epoch 9/20\n",
            "220/221 [============================>.] - ETA: 0s - loss: 0.2716 - accuracy: 0.9082\n",
            "Epoch 00009: val_accuracy did not improve from 0.88568\n",
            "221/221 [==============================] - 4s 20ms/step - loss: 0.2712 - accuracy: 0.9085 - val_loss: 0.5235 - val_accuracy: 0.8795\n",
            "Epoch 10/20\n",
            "220/221 [============================>.] - ETA: 0s - loss: 0.2648 - accuracy: 0.9104\n",
            "Epoch 00010: val_accuracy did not improve from 0.88568\n",
            "221/221 [==============================] - 4s 20ms/step - loss: 0.2641 - accuracy: 0.9106 - val_loss: 0.5026 - val_accuracy: 0.8795\n",
            "Epoch 11/20\n",
            "220/221 [============================>.] - ETA: 0s - loss: 0.2621 - accuracy: 0.9072\n",
            "Epoch 00011: val_accuracy did not improve from 0.88568\n",
            "221/221 [==============================] - 4s 20ms/step - loss: 0.2623 - accuracy: 0.9072 - val_loss: 0.5131 - val_accuracy: 0.8783\n",
            "Epoch 12/20\n",
            "220/221 [============================>.] - ETA: 0s - loss: 0.2585 - accuracy: 0.9104\n",
            "Epoch 00012: val_accuracy did not improve from 0.88568\n",
            "221/221 [==============================] - 4s 20ms/step - loss: 0.2578 - accuracy: 0.9106 - val_loss: 0.5394 - val_accuracy: 0.8755\n",
            "Epoch 13/20\n",
            "220/221 [============================>.] - ETA: 0s - loss: 0.2593 - accuracy: 0.9068\n",
            "Epoch 00013: val_accuracy did not improve from 0.88568\n",
            "221/221 [==============================] - 4s 19ms/step - loss: 0.2588 - accuracy: 0.9070 - val_loss: 0.5159 - val_accuracy: 0.8823\n",
            "Epoch 00013: early stopping\n",
            "Model: \"NN_sequential_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "image (InputLayer)           [(None, 489)]             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1000)              490000    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1000)              1001000   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 261)               261261    \n",
            "=================================================================\n",
            "Total params: 1,752,261\n",
            "Trainable params: 1,752,261\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPNvVLv1ymql",
        "colab_type": "code",
        "outputId": "4a840e11-68a2-4e42-fbf1-56979a09201c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "adv_model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "                   metrics=['acc'])\n",
        "es=tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=10)\n",
        "mc = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_categorical_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "print(\"applied adversarial regularization on base neural network\")\n",
        "#adv_model.compile(optimizer='adam', loss='categorical_cross_entropy', metrics=['accuracy'])\n",
        "adv_model.fit(train_set_for_adv_model,validation_split=0.2 ,epochs=15,callbacks=[es,mc])"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "applied adversarial regularization on base neural network\n",
            "Epoch 1/15\n",
            "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
            "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int64\n",
            "WARNING:tensorflow:Cannot perturb feature image\n",
            "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
            "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int64\n",
            "WARNING:tensorflow:Cannot perturb feature image\n",
            "221/221 [==============================] - ETA: 0s - loss: 1.8495 - categorical_crossentropy: 1.5413 - categorical_accuracy: 0.7349 - adversarial_loss: 1.5413WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
            "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int64\n",
            "WARNING:tensorflow:Cannot perturb feature image\n",
            "\n",
            "Epoch 00001: val_categorical_accuracy improved from -inf to 0.85795, saving model to best_model.h5\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 1.8495 - categorical_crossentropy: 1.5413 - categorical_accuracy: 0.7349 - adversarial_loss: 1.5413 - val_loss: 0.8672 - val_categorical_crossentropy: 0.7227 - val_categorical_accuracy: 0.8580 - val_adversarial_loss: 0.7227\n",
            "Epoch 2/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.6026 - categorical_crossentropy: 0.5022 - categorical_accuracy: 0.8861 - adversarial_loss: 0.5022\n",
            "Epoch 00002: val_categorical_accuracy improved from 0.85795 to 0.87832, saving model to best_model.h5\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.6026 - categorical_crossentropy: 0.5022 - categorical_accuracy: 0.8861 - adversarial_loss: 0.5022 - val_loss: 0.6341 - val_categorical_crossentropy: 0.5284 - val_categorical_accuracy: 0.8783 - val_adversarial_loss: 0.5284\n",
            "Epoch 3/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.4426 - categorical_crossentropy: 0.3688 - categorical_accuracy: 0.9005 - adversarial_loss: 0.3688\n",
            "Epoch 00003: val_categorical_accuracy improved from 0.87832 to 0.88398, saving model to best_model.h5\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.4426 - categorical_crossentropy: 0.3688 - categorical_accuracy: 0.9005 - adversarial_loss: 0.3688 - val_loss: 0.6199 - val_categorical_crossentropy: 0.5166 - val_categorical_accuracy: 0.8840 - val_adversarial_loss: 0.5166\n",
            "Epoch 4/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3922 - categorical_crossentropy: 0.3269 - categorical_accuracy: 0.9055 - adversarial_loss: 0.3269\n",
            "Epoch 00004: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.3922 - categorical_crossentropy: 0.3269 - categorical_accuracy: 0.9055 - adversarial_loss: 0.3269 - val_loss: 0.6243 - val_categorical_crossentropy: 0.5202 - val_categorical_accuracy: 0.8766 - val_adversarial_loss: 0.5202\n",
            "Epoch 5/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3695 - categorical_crossentropy: 0.3079 - categorical_accuracy: 0.9045 - adversarial_loss: 0.3079\n",
            "Epoch 00005: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 30ms/step - loss: 0.3695 - categorical_crossentropy: 0.3079 - categorical_accuracy: 0.9045 - adversarial_loss: 0.3079 - val_loss: 0.6183 - val_categorical_crossentropy: 0.5152 - val_categorical_accuracy: 0.8823 - val_adversarial_loss: 0.5152\n",
            "Epoch 6/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3468 - categorical_crossentropy: 0.2890 - categorical_accuracy: 0.9090 - adversarial_loss: 0.2890\n",
            "Epoch 00006: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.3468 - categorical_crossentropy: 0.2890 - categorical_accuracy: 0.9090 - adversarial_loss: 0.2890 - val_loss: 0.5916 - val_categorical_crossentropy: 0.4930 - val_categorical_accuracy: 0.8812 - val_adversarial_loss: 0.4930\n",
            "Epoch 7/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3380 - categorical_crossentropy: 0.2816 - categorical_accuracy: 0.9085 - adversarial_loss: 0.2816\n",
            "Epoch 00007: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 30ms/step - loss: 0.3380 - categorical_crossentropy: 0.2816 - categorical_accuracy: 0.9085 - adversarial_loss: 0.2816 - val_loss: 0.5980 - val_categorical_crossentropy: 0.4983 - val_categorical_accuracy: 0.8823 - val_adversarial_loss: 0.4983\n",
            "Epoch 8/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3307 - categorical_crossentropy: 0.2756 - categorical_accuracy: 0.9100 - adversarial_loss: 0.2756\n",
            "Epoch 00008: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.3307 - categorical_crossentropy: 0.2756 - categorical_accuracy: 0.9100 - adversarial_loss: 0.2756 - val_loss: 0.5968 - val_categorical_crossentropy: 0.4973 - val_categorical_accuracy: 0.8812 - val_adversarial_loss: 0.4973\n",
            "Epoch 9/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3271 - categorical_crossentropy: 0.2725 - categorical_accuracy: 0.9063 - adversarial_loss: 0.2725\n",
            "Epoch 00009: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 30ms/step - loss: 0.3271 - categorical_crossentropy: 0.2725 - categorical_accuracy: 0.9063 - adversarial_loss: 0.2725 - val_loss: 0.6140 - val_categorical_crossentropy: 0.5117 - val_categorical_accuracy: 0.8800 - val_adversarial_loss: 0.5117\n",
            "Epoch 10/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3194 - categorical_crossentropy: 0.2662 - categorical_accuracy: 0.9089 - adversarial_loss: 0.2662\n",
            "Epoch 00010: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.3194 - categorical_crossentropy: 0.2662 - categorical_accuracy: 0.9089 - adversarial_loss: 0.2662 - val_loss: 0.6442 - val_categorical_crossentropy: 0.5368 - val_categorical_accuracy: 0.8806 - val_adversarial_loss: 0.5368\n",
            "Epoch 11/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3189 - categorical_crossentropy: 0.2658 - categorical_accuracy: 0.9063 - adversarial_loss: 0.2658\n",
            "Epoch 00011: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.3189 - categorical_crossentropy: 0.2658 - categorical_accuracy: 0.9063 - adversarial_loss: 0.2658 - val_loss: 0.6235 - val_categorical_crossentropy: 0.5195 - val_categorical_accuracy: 0.8806 - val_adversarial_loss: 0.5195\n",
            "Epoch 12/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3141 - categorical_crossentropy: 0.2618 - categorical_accuracy: 0.9058 - adversarial_loss: 0.2618\n",
            "Epoch 00012: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 30ms/step - loss: 0.3141 - categorical_crossentropy: 0.2618 - categorical_accuracy: 0.9058 - adversarial_loss: 0.2618 - val_loss: 0.6167 - val_categorical_crossentropy: 0.5139 - val_categorical_accuracy: 0.8834 - val_adversarial_loss: 0.5139\n",
            "Epoch 13/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3111 - categorical_crossentropy: 0.2593 - categorical_accuracy: 0.9070 - adversarial_loss: 0.2593\n",
            "Epoch 00013: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.3111 - categorical_crossentropy: 0.2593 - categorical_accuracy: 0.9070 - adversarial_loss: 0.2593 - val_loss: 0.6124 - val_categorical_crossentropy: 0.5103 - val_categorical_accuracy: 0.8795 - val_adversarial_loss: 0.5103\n",
            "Epoch 14/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.3032 - categorical_crossentropy: 0.2526 - categorical_accuracy: 0.9110 - adversarial_loss: 0.2526\n",
            "Epoch 00014: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.3032 - categorical_crossentropy: 0.2526 - categorical_accuracy: 0.9110 - adversarial_loss: 0.2526 - val_loss: 0.6401 - val_categorical_crossentropy: 0.5334 - val_categorical_accuracy: 0.8789 - val_adversarial_loss: 0.5334\n",
            "Epoch 15/15\n",
            "221/221 [==============================] - ETA: 0s - loss: 0.2996 - categorical_crossentropy: 0.2496 - categorical_accuracy: 0.9082 - adversarial_loss: 0.2496\n",
            "Epoch 00015: val_categorical_accuracy did not improve from 0.88398\n",
            "221/221 [==============================] - 7s 31ms/step - loss: 0.2996 - categorical_crossentropy: 0.2496 - categorical_accuracy: 0.9082 - adversarial_loss: 0.2496 - val_loss: 0.6326 - val_categorical_crossentropy: 0.5271 - val_categorical_accuracy: 0.8812 - val_adversarial_loss: 0.5271\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff7259134a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    }
  ]
}